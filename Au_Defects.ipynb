{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bb3dd3-6d06-4f0a-914c-ea206f4ef57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import math\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "#from torchsummary import summary\n",
    "#from tqdm import tqdm\n",
    "fp_data = \"Au\\Dataset\\Au_defect_dataset.h5\"\n",
    "fp_metadata = \"Au\\Metadata\\Au_defect_metadata.pkl\"\n",
    "fp_metadata1 = \"Au\\Metadata\\Au_clean_metadata.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291447d1-1d52-4848-9004-83ca5e56d2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File(fp_data, 'r')\n",
    "defect_data = np.array(f['defect_data'])\n",
    "clean_data = np.array(f['clean_data'])#[:,:256,:256]\n",
    "f.close() # important to close the file for h5 files before ending the script/scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da433e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data):\n",
    "    # Reshape the data into a 2D array\n",
    "    nsamples, nx, ny = data.shape\n",
    "    data = data.reshape((nsamples, nx*ny))\n",
    "    \n",
    "    # Normalize the data\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(data)\n",
    "    data = scaler.transform(data)\n",
    "    \n",
    "    return data, scaler\n",
    "\n",
    "def split_and_transform(data, scaler, split):\n",
    "    # Normalize the data using the scaler\n",
    "    data = scaler.transform(data)\n",
    "    \n",
    "    # Split the data into training, validation, and testing sets\n",
    "    N_data = data.shape[0]\n",
    "    N_train = int(N_data * split[0])\n",
    "    N_val = int(N_data * split[1])\n",
    "    N_test = int((N_data *split[2]))\n",
    "    \n",
    "    train_data = data[:N_train, :]\n",
    "    val_data = data[N_train:N_train+N_val, :]\n",
    "    test_data = data[N_train+N_val:N_train+N_val+N_test, :]\n",
    "    \n",
    "    # Transform the data into PyTorch tensors and reshape\n",
    "    train_data = torch.from_numpy(train_data).to(torch.float32).reshape(-1, 512, 512)[:, None, :, :]\n",
    "    val_data = torch.from_numpy(val_data).to(torch.float32).reshape(-1, 512, 512)[:, None, :, :]\n",
    "    test_data =  torch.from_numpy(test_data).to(torch.float32).reshape(-1, 512, 512)[:, None, :, :]\n",
    "    \n",
    "    \n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "# Normalize the defect data\n",
    "defect_data, scaler = normalize(defect_data)\n",
    "\n",
    "# Splitting the data into training, validation, and testing sets\n",
    "defect_train_split, defect_val_split, defect_test_split = 0.06, 0.02, 0.02\n",
    "\n",
    "clean_train_split, clean_val_split, clean_test_split = 0.06, 0.02, 0.02\n",
    "\n",
    "# Split and transform the defect data\n",
    "\n",
    "defect_train, defect_val, defect_test = split_and_transform(defect_data, scaler, (defect_train_split, defect_val_split, defect_test_split))\n",
    "\n",
    "# Normalize the clean data\n",
    "clean_data, scaler = normalize(clean_data)\n",
    "\n",
    "# Split and transform the clean data\n",
    "clean_train, clean_val, clean_test = split_and_transform(clean_data, scaler, (clean_train_split, clean_val_split, clean_test_split))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09510c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "defect_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43e246f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xy(defect_data, clean_data):\n",
    "    N = len(defect_data)\n",
    "    X = np.concatenate([-1*defect_data, 1*clean_data], axis=0)\n",
    "    Y = np.zeros((2*N, 2), dtype = 'float32')\n",
    "    Y[:N, 1] = 1\n",
    "    Y[N:, 0] = 1\n",
    "    return X, Y\n",
    "\n",
    "class DefectCountDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None, target_transform=None):\n",
    "        self.img = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.img[idx,:,:]\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label\n",
    "\n",
    "X_train , Y_train = get_xy(defect_train, clean_train)\n",
    "X_val, Y_val = get_xy(defect_val, clean_val)\n",
    "X_test, Y_test = get_xy(defect_test, clean_test)\n",
    "defect_train_dataset = DefectCountDataset(X_train, Y_train)\n",
    "train_dataloader = DataLoader(defect_train_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "\n",
    "defect_val_dataset = DefectCountDataset(X_val, Y_val)\n",
    "val_dataloader = DataLoader(defect_val_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "\n",
    "defect_test_dataset = DefectCountDataset(X_test, Y_test)\n",
    "test_dataloader = DataLoader(defect_test_dataset, batch_size=128, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3a969d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(torch.nn.Module):\n",
    "    def forward(self, input):\n",
    "        if len(input.shape) < 4:\n",
    "            input = input[None,:,:,:]\n",
    "           \n",
    "        else:\n",
    "            input = input\n",
    "        #input = input.reshape(input.size(0), -1)\n",
    "        return input.reshape(-1, input.size(1)*input.size(2)*input.size(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fac197",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ClassificationNN(nn.Module):\n",
    "    def __init__(self, input_channel, num_classes, num_conv_layers, conv_output_channels, kernel_size, stride, padding, \n",
    "                 activations,dropout_probs, use_batchnorm, use_maxpool,Maxpool_size, Height, Width):\n",
    "        super(ClassificationNN, self).__init__()\n",
    "        \n",
    "        # Initialize list of layers\n",
    "        layers = []\n",
    "        \n",
    "        # Create convolutional layers\n",
    "        for i in range(num_conv_layers):\n",
    "            # Create convolutional layer\n",
    "            conv = nn.Conv2d(in_channels=input_channel,out_channels=conv_output_channels[i],kernel_size=kernel_size,stride =stride,padding=padding)\n",
    "            # Initialize weights using Kaiming initialization\n",
    "            nn.init.kaiming_normal_(conv.weight)\n",
    "            input_channel = conv_output_channels[i]\n",
    "            \n",
    "            # Add convolutional layer to model\n",
    "            layers.append(conv)\n",
    "            \n",
    "            # Add activation function\n",
    "            layers.append(activations[i])\n",
    "\n",
    "            # Calulate what the final dimensions will be (w/o Maxpool, see below)\n",
    "            New_Height = ((Height - kernel_size + 2*padding)//stride + 1)\n",
    "\n",
    "            New_Width = ((Width - kernel_size + 2*padding)//stride + 1)\n",
    "\n",
    "            # Add dropout layer if specified\n",
    "            if dropout_probs[i] > 0:\n",
    "                dropout_layer = nn.Dropout(p=dropout_probs[i])\n",
    "                layers.append(dropout_layer)\n",
    "\n",
    "            # Add batch normalization layer if specified\n",
    "            if use_batchnorm[i]:\n",
    "                layers.append(nn.BatchNorm2d(conv_output_channels[i]))\n",
    "            \n",
    "            # Add max pooling layer if specified\n",
    "            if use_maxpool[i]:\n",
    "                layers.append(nn.MaxPool2d(Maxpool_size))\n",
    "                # Calulate what the final dimensions will be (if there's a maxpool)\n",
    "                New_Height = New_Height  // Maxpool_size\n",
    "                New_Width = New_Width // Maxpool_size\n",
    "\n",
    "            # Update the final dimensions values\n",
    "            Height = New_Height\n",
    "            Width = New_Width\n",
    "            #print(Width)\n",
    "                    \n",
    "        # Flatten layer\n",
    "        layers.append(Flatten())\n",
    "\n",
    "        # Add fully connected layer\n",
    "        layers.append(nn.Linear( conv_output_channels[i] * (New_Height*New_Width), num_classes))\n",
    "        \n",
    "        # Add softmax activation function\n",
    "        layers.append(nn.Softmax(dim=1))\n",
    "        \n",
    "        # Create Sequential model\n",
    "        self.Conv_layers = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.Conv_layers(x)\n",
    "\n",
    "# Define model\n",
    "model = ClassificationNN(input_channel=1, num_classes=2, num_conv_layers=6, conv_output_channels = [16, 24, 30, 32, 32, 32], kernel_size = 5, stride = 1,\n",
    "                         padding = 1,activations=[nn.ReLU(), nn.ReLU(), nn.ReLU(),nn.Tanh(),nn.Tanh(),nn.Tanh()],dropout_probs = [0.1, 0.2, 0.4, 0.4, 0.2, 0.1], use_batchnorm=[False, False, False, False, False, False], \n",
    "                         use_maxpool=[True, False, True, False, False, False], Maxpool_size = 2, Height = 512, Width = 512)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7bf627",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=9e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351a0434",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import StepLR\n",
    "# Initialize empty lists to store training and validation losses\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "learning_rates = []\n",
    "# Set the number of epochs to train the model\n",
    "n_epochs = 15\n",
    "\n",
    "# Define the learning rate scheduler\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "# Loop over the number of epochs\n",
    "for epoch in range(1, n_epochs+1): \n",
    "    # Initialize running training and validation losses to 0\n",
    "    running_loss_train = 0.0\n",
    "    running_loss_val = 0.0\n",
    "    \n",
    "    # Set the model to train mode\n",
    "    model.train()\n",
    "    \n",
    "    # Loop over the training data\n",
    "    for train_data in train_dataloader:\n",
    "        # Clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # Retrieve images and image labels from the dataloader\n",
    "        train_images, train_labels = train_data\n",
    "        # Forward pass: compute predicted outputs by passing inputs to the model\n",
    "        train_outputs = model(train_images)\n",
    "        # Calculate the loss\n",
    "        loss = criterion(train_outputs, train_labels)\n",
    "        # Backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # Perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # Update the running training loss\n",
    "        running_loss_train += loss.item() \n",
    "        \n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()  \n",
    "    \n",
    "    # Loop over the validation data with torch.no_grad() to disable gradient calculations\n",
    "    with torch.no_grad():\n",
    "        for val_data in val_dataloader:\n",
    "            # Retrieve images and image labels from the dataloader\n",
    "            val_images, val_labels = val_data\n",
    "            # Forward pass: compute predicted outputs by passing inputs to the model\n",
    "            val_outputs = model(val_images)\n",
    "            # Calculate the loss\n",
    "            loss = criterion(val_outputs, val_labels)\n",
    "            # Update the running validation loss\n",
    "            running_loss_val += loss.item()\n",
    "        \n",
    "    # Calculate and store the average training loss for the epoch\n",
    "    running_loss_train /= len(train_dataloader)\n",
    "    train_loss.append(running_loss_train)\n",
    "    \n",
    "    # Calculate and store the average validation loss for the epoch\n",
    "    running_loss_val /= len(val_dataloader)\n",
    "    val_loss.append(running_loss_val)\n",
    "\n",
    "    # retrieve the learning rate at the current epoch\n",
    "    lr = optimizer.param_groups[0]['lr']\n",
    "    # store the learning rate in a list\n",
    "    learning_rates.append(lr)\n",
    "\n",
    "    # Print the average training and validation losses for the epoch\n",
    "    print('Epoch: {}/{} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f} \\tlr: {:.10f}'.format(\n",
    "        epoch, \n",
    "        n_epochs,\n",
    "        running_loss_train,\n",
    "        running_loss_val,\n",
    "        lr\n",
    "        ))\n",
    "    \n",
    "\n",
    "    # Step the learning rate scheduler\n",
    "    scheduler.step()\n",
    "    \n",
    "# Plot the training and validation losses\n",
    "plt.plot(range(1,n_epochs+1), train_loss, 'g', label='Training loss')\n",
    "plt.plot(range(1,n_epochs+1), val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96412c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# initialize variables to store the total number of correct predictions and total number of predictions\n",
    "correct = 0\n",
    "total = 0\n",
    "N_images = len(defect_test)*2\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in test_dataloader:\n",
    "        images, labels = data\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = model(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data,1)\n",
    "        _, label = torch.max(labels.data,1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == label).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the {N_images} test images: {100 * correct // total} %')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cac034",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee4b894",
   "metadata": {},
   "outputs": [],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41ab219",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "486fc8ce17fb909ebc26a9549a8e9ee69b34e1182be5d7f510e3ad8af0727554"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
